import json
import asyncio
from fastmcp import Client
from openai import AsyncOpenAI


async def ask_llm_and_get_response(llm_client, messages, available_tools):
    """LLMì—ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ê¸°"""
    
    # LLMì—ê²Œ ì§ˆë¬¸ ë³´ë‚´ê¸°
    response = await llm_client.chat.completions.create(
        model="gpt-4o",
        max_tokens=1000,
        messages=messages,
        tools=available_tools
    )
    
    # LLMì˜ ë‹µë³€ì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
    llm_message = {
        "role": "assistant",
        "content": response.choices[0].message.content,
        "tool_calls": response.choices[0].message.tool_calls
    }
    messages.append(llm_message)
    
    # LLMì´ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€í–ˆë‹¤ë©´ í™”ë©´ì— ì¶œë ¥
    if response.choices[0].message.content:
        print("[ğŸ¤– LLM ë‹µë³€] ", response.choices[0].message.content)
    
    return response


async def run_tools_and_get_results(llm_response, mcp_server, messages):
    """LLMì´ ìš”ì²­í•œ ë„êµ¬ë“¤ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì–»ê¸°"""
    
    # LLMì´ ë„êµ¬ ì‚¬ìš©ì„ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
    tool_calls = llm_response.choices[0].message.tool_calls
    if not tool_calls:
        return
    
    # ìš”ì²­ëœ ê° ë„êµ¬ë¥¼ ì‹¤í–‰
    for tool_call in tool_calls:
        tool_name = tool_call.function.name
        tool_arguments = tool_call.function.arguments
        
        print(f"[ğŸ”§ ë„êµ¬ ì‹¤í–‰] {tool_name}({tool_arguments})")
        
        # ë„êµ¬ ì‹¤í–‰í•˜ê¸°
        arguments = json.loads(tool_arguments)
        result = await mcp_server.call_tool(tool_name, arguments)
        
        print(f"[âœ… ì‹¤í–‰ ê²°ê³¼] {result}")
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
        tool_result_message = {
            "role": "tool",
            "tool_call_id": tool_call.id,
            "content": str(result)
        }
        messages.append(tool_result_message)


def convert_mcp_tools_to_openai_format(mcp_tools):
    """MCP ë„êµ¬ë“¤ì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    openai_tools = []
    
    for tool in mcp_tools:
        openai_tool = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema if hasattr(tool, 'inputSchema') else {}
            }
        }
        openai_tools.append(openai_tool)
    
    return openai_tools


def check_if_llm_wants_to_use_tools(llm_response):
    """LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì–´í•˜ëŠ”ì§€ í™•ì¸"""
    return llm_response.choices[0].message.tool_calls is not None


async def main():
    """ì‚¬ìš©ìì™€ LLMì´ ëŒ€í™”í•˜ëŠ” ë©”ì¸ í”„ë¡œê·¸ë¨"""
    
    # OpenLLM í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„
    llm_client = AsyncOpenAI()
    
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    conversation_history = []
    
    # MCP ì„œë²„ì— ì—°ê²°
    async with Client("http://127.0.0.1:8000/mcp") as mcp_server:
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ ê°€ì ¸ì˜¤ê¸°
        mcp_tools = await mcp_server.list_tools()
        
        # MCP ë„êµ¬ë“¤ì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        openai_tools = convert_mcp_tools_to_openai_format(mcp_tools)
        
        # ì‚¬ìš©ìì™€ ê³„ì† ëŒ€í™”í•˜ê¸°
        while True:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_message = input("[ğŸ‘¤ ì‚¬ìš©ì] ")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            conversation_history.append({
                "role": "user", 
                "content": user_message
            })
            
            # LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µ
            while True:
                # LLMì—ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
                llm_response = await ask_llm_and_get_response(
                    llm_client, 
                    conversation_history, 
                    openai_tools
                )
                
                # LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì–´í•œë‹¤ë©´
                if check_if_llm_wants_to_use_tools(llm_response):
                    # ë„êµ¬ë“¤ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì–»ê¸°
                    await run_tools_and_get_results(
                        llm_response, 
                        mcp_server, 
                        conversation_history
                    )
                else:
                    # LLMì´ ë” ì´ìƒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë‹ˆ ëŒ€í™” í„´ ì¢…ë£Œ
                    break


if __name__ == '__main__':
    asyncio.run(main())